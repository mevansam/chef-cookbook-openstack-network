<%= node['openstack']['network']['custom_template_banner'] %>
<% if @hostname && !@hostname.empty? -%>
[DEFAULT]
host = <%= @hostname %>

<% end -%>
[ml2]
# (ListOpt) List of network type driver entrypoints to be loaded from
# the neutron.ml2.type_drivers namespace.
#
# type_drivers = local,flat,vlan,gre,vxlan
# Example: type_drivers = flat,vlan,gre,vxlan
type_drivers = <%= node['openstack']['network']['ml2']['type_drivers'] %>

# (ListOpt) Ordered list of network_types to allocate as tenant
# networks. The default value 'local' is useful for single-box testing
# but provides no connectivity between hosts.
#
# tenant_network_types = local
# Example: tenant_network_types = vlan,gre,vxlan
tenant_network_types = <%= node['openstack']['network']['ml2']['tenant_network_types'] %>

# (ListOpt) Ordered list of networking mechanism driver entrypoints
# to be loaded from the neutron.ml2.mechanism_drivers namespace.
# mechanism_drivers =
# Example: mechanism_drivers = arista
# Example: mechanism_drivers = cisco,logger
mechanism_drivers = <%= node['openstack']['network']['ml2']['mechanism_drivers'] %>

[ml2_type_flat]
# (ListOpt) List of physical_network names with which flat networks
# can be created. Use * to allow flat networks with arbitrary
# physical_network names.
#
# flat_networks =
# Example:flat_networks = physnet1,physnet2
# Example:flat_networks = *
flat_networks = <%= node['openstack']['network']['ml2']['flat_networks'] %>

[ml2_type_vlan]
# (ListOpt) List of <physical_network>[:<vlan_min>:<vlan_max>] tuples
# specifying physical_network names usable for VLAN provider and
# tenant networks, as well as ranges of VLAN tags on each
# physical_network available for allocation as tenant networks.
#
# network_vlan_ranges =
# Example: network_vlan_ranges = physnet1:1000:2999,physnet2
<% if @network_vlan_ranges -%>
network_vlan_ranges = <%= @network_vlan_ranges %>
<% else -%>
network_vlan_ranges = <%= node['openstack']['network']['ml2']['network_vlan_ranges'] %>
<% end -%>

[ml2_type_gre]
# (ListOpt) Comma-separated list of <tun_min>:<tun_max> tuples enumerating ranges of GRE tunnel IDs that are available for tenant network allocation
tunnel_id_ranges = <%= node['openstack']['network']['ml2']['tunnel_id_ranges'] %>

[ml2_type_vxlan]
# (ListOpt) Comma-separated list of <vni_min>:<vni_max> tuples enumerating
# ranges of VXLAN VNI IDs that are available for tenant network allocation.
vni_ranges = <%= node['openstack']['network']['ml2']['vni_ranges'] %>

# (StrOpt) Multicast group for the VXLAN interface. When configured, will
# enable sending all broadcast traffic to this multicast group. When left
# unconfigured, will disable multicast VXLAN mode.
#
# vxlan_group =
# Example: vxlan_group = 239.1.1.1
vxlan_group = <%= node['openstack']['network']['ml2']['vxlan_group'] %>

<% if node['openstack']['network']['ml2']['mechanism_drivers'].split(",").include?("openvswitch") -%>
[ovs]
# Indicates if this agent runs alongside other OVS agents. If set it will
# ensure that this agent will not attempt to reconfigure the l2 switches.
#multi = False

# Do not change this parameter unless you have a good reason to.
# This is the name of the OVS integration bridge. There is one per hypervisor.
# The integration bridge acts as a virtual "patch bay". All VM VIFs are
# attached to this bridge and then "patched" according to their network
# connectivity.
#
# integration_bridge = br-int
<% if @integration_bridge -%>
integration_bridge = <%= @integration_bridge %>
<% elsif node["openstack"]["network"]["openvswitch"]["integration_bridge"] -%>
integration_bridge = <%= node["openstack"]["network"]["openvswitch"]["integration_bridge"] %>
<% end -%>

# Only used for the agent if tunnel_id_ranges (above) is not empty for
# the server.  In most cases, the default value should be fine.
#
# tunnel_bridge = br-tun
<% if @tunnel_bridge -%>
tunnel_bridge = <%= @tunnel_bridge %>
<% elsif node["openstack"]["network"]["openvswitch"]["tunnel_bridge"] -%>
tunnel_bridge = <%= node["openstack"]["network"]["openvswitch"]["tunnel_bridge"] %>
<% end -%>

# Peer patch port in integration bridge for tunnel bridge
# int_peer_patch_port = patch-tun
<% if node["openstack"]["network"]["openvswitch"]["int_peer_patch_port"] -%>
int_peer_patch_port = <%= node["openstack"]["network"]["openvswitch"]["int_peer_patch_port"] %>
<% end -%>

# Peer patch port in tunnel bridge for integration bridge
# tun_peer_patch_port = patch-int
<% if node["openstack"]["network"]["openvswitch"]["tun_peer_patch_port"] -%>
tun_peer_patch_port = <%= node["openstack"]["network"]["openvswitch"]["tun_peer_patch_port"] %>
<% end -%>

# Uncomment this line for the agent if tunnel_id_ranges (above) is not
# empty for the server. Set local-ip to be the local IP address of
# this hypervisor.
#
# local_ip =
local_ip = <%= @local_ip %>

# (ListOpt) Comma-separated list of <physical_network>:<bridge> tuples
# mapping physical network names to the agent's node-specific OVS
# bridge names to be used for flat and VLAN networks. The length of
# bridge names should be no more than 11. Each bridge must
# exist, and should have a physical network interface configured as a
# port. All physical networks listed in network_vlan_ranges on the
# server should have mappings to appropriate bridges on each agent.
#
# bridge_mappings =
<% if @bridge_mappings -%>
bridge_mappings = <%= @bridge_mappings %>
<% elsif node["openstack"]["network"]["openvswitch"]["bridge_mappings"] -%>
bridge_mappings = <%= node["openstack"]["network"]["openvswitch"]["bridge_mappings"] %>
<% end -%>

[agent]
<% if @root_helper -%>
# Override of root_helper defined in neutron.conf
root_helper = <%= @root_helper %>
<% end -%>

# Agent's polling interval in seconds
polling_interval = <%= node["openstack"]["network"]["openvswitch"]["polling_interval"] %>

# (ListOpt) The types of tenant network tunnels supported by the agent.
# Setting this will enable tunneling support in the agent. This can be set to
# either 'gre' or 'vxlan'. If this is unset, it will default to [] and
# disable tunneling support in the agent. When running the agent with the OVS
# plugin, this value must be the same as "tunnel_type" in the "[ovs]" section.
# When running the agent with ML2, you can specify as many values here as
# your compute hosts supports.
#
# tunnel_types =
# Example: tunnel_types = gre
# Example: tunnel_types = vxlan
# Example: tunnel_types = vxlan,gre
<% if @tunnel_types -%>
tunnel_types = <%= @tunnel_types %>
<% end -%>

# (IntOpt) This is the MTU size of veth interfaces.
# Do not change unless you have a good reason to.
# The default MTU size of veth interfaces is 1500.
# Example: veth_mtu = 1504
veth_mtu = <%= node["openstack"]["network"]["openvswitch"]["veth_mtu"] %>

# (BoolOpt) Flag to enable l2-population extension. This option should only be
# used in conjunction with ml2 plugin and l2population mechanism driver. It'll
# enable plugin to populate remote ports macs and IPs (using fdb_add/remove
# RPC calbbacks instead of tunnel_sync/update) on OVS agents in order to
# optimize tunnel management.
#
# l2_population = False
l2_population = <%= node["openstack"]["network"]["openvswitch"]["l2_population"] %>
 
[securitygroup]
# Firewall driver for realizing neutron security group function
# Default: firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
<% if @fw_driver -%>
firewall_driver = <%= @fw_driver %>
<% else -%>
firewall_driver = <%= node["openstack"]["network"]["openvswitch"]["fw_driver"] %>
<% end -%>

# Controls if neutron security group is enabled or not.
# It should be false when you use nova security group.
<% if @enable_security_group -%>
enable_security_group = <%= @enable_security_group %>
<% else -%>
enable_security_group = <%= node["openstack"]["network"]["openvswitch"]["enable_security_group"] %>
<% end -%>
<% else -%>
[securitygroup]
# Controls if neutron security group is enabled or not.
# It should be false when you use nova security group.
enable_security_group = <%= node['openstack']['network']['ml2']['enable_security_group'] %>
<% end -%>
